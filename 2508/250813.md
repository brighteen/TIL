16진수 2진수

Q2. Autoencoder의 Latent Space에 정말 모든 정보가 다 담겨있을까? Latent vector라고 설정한  $z^{(e)}$보다  $z^{(e+1)}$또는 $z^{(k)}$에 더 많은 정보가 담겨져 있는 것은 아닐까?
AE는 잠재 공간에 데이터의 모든 정보를 압축시키려고 학습함. $z^{(e+1)}$는 결국 압축된 정보를 펼치는 꼴임.
데이터 처리 부등식에 의해 정보를 가공하면 할수록(e-> e+1 -> k) 정보의 손실이 일어남.

Q3. 비선형변환을 없앤 AE의 잠재 공간은 PCA(선형차원축소)에 의한 차원 축소와 어떤 관계가 있을까?
PCA는 데이터의 분산을 가장 크게 보존하는 주성분을 찾음. 이는 데이터를 새로운 축에 투영했을 때 발생하는 재구성 오류를 최소화 하는 것.
선형 AE도 마찬가지로 재구성 오차를 최소화하는 것을 목표로 함.
선형AE의 잠재공간이 정의하는 부분 공간은 PCA의 첫번째 주성분이 정의하는 부분 공간과 동일.
기저 벡터(축) : 직교할 필요 없음 / 반드시 직교

변분 오토인코더 Variational AE
변분 추론 variational inference
어떤 확률분포 p를 찾기 위해 어떤 유연한 unconditional 분포 q를 가정해서 p와 근사하는 추론 방법
저차원 확률 분포를 근사적으로 추정하자
잠재 공간에서의 확률분포함수는 $p(z^{(e)}|z^{(1)})$인데 이를 계산하려면 결국 고차원 확률 분포 $p(e^{(1)})$를 알아야 함.
그래서 어떤 유연하고 unconditional한 확률분포 q를 가정하고 그 확률 분포를 $p(z^{(e)}|z^{(1)})$에 근사하자가 VAE의 아이디어.
이때 q는 보통 정규분포로 가정.(Beta, gamma분포는 z값이 제한적)
잘 찾은 q로 N개 데이터를 샘플링해 디코더를 통해 복원.
여기에 전제되어야 하는 것은 재구성 오차, 복원 데이터와 입력 데이터의 차이가 작아야 함.
VAE $Loss_{minimize}$ = 재구성 오차 + $D_{KL}(q||p)$
