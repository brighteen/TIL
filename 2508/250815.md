AE의 인코더를 통해 저차원으로 차원축소된 개와 고양이는 같은 저차원 공간에 존재함
- 개와 고양이의 핵심 특징들을 추출해 저차원 잠재 공간에 매핑됨.
- 하지만 AE에서의 저차원 잠재공간에서는 둘이 연결되있진 않음.
- 인코더는 입력 데이터를 저차원 잠재공간의 한 점으로 매핑하고, 디코더는 그 점에서 원본 데이터를 복원하는 훈련만 함. -> 개와 고양이는 다른 '군집'을 이루는 섬처럼 흩어짐.
- 두 군집 사이에 텅빈 공간에 점을 찍고 디코더로 복원하면 아무 의미없는 노이즈나 깨진 데이터를 생성할 뿐. -> 이때 개와 고양이는 **불연속**적임.

그럼 개와 고양이를 저차원 잠재공간에서 연결시키려면?
VAE는 데이터를 저차원 잠재 공간에 하나의 점이 아닌 약간의 범위를 가진 '**확률 분포**'로 매핑.
- ==각 데이터를 확률 분포로 매핑한다는게 뭔지?==
	- $Loss_{VAE}$ = 재구성오차 + $D_{KL}$ 이때 $D_{KL}$이라는 규제항이 인코더가 만들어내는 모든 확률 분포들이 표준정규분포($\mu =0, \sigma=1$)에 가깝도록 강제함.
- 이 규제항 때문에 각 군집(개, 고양이)는 **원점 근처로 모이도록 압박을 받음**.(군집들이 멀리 떨어져 있을 수록 $D_{KL}$손실이 커지기 때문 -> 분포들이 어느정도 겹치도록 배치됨.)
- 이 분포들이 서로 겹치도록 학습하기에 군집 사이에 빈틈이 사라지고 전체 공간(저차원 잠재 공간)이 부드럽게 이어짐.
- 개와 고양이의 각 점을 선택하고 그 사이를 잇는 직선 경로를 따라 이동하며 디코더로 복원하면 개와 고양이의 중간 형태를 띄는 이미지가 생성됨.
- > 저차원 잠재 공간에서 데이터들이 **연속성**을 가짐.
- > 이러한 과정을 잠재 공간 보간(Latent space Interpolation)이라고 함.

==생성 모델을 학습시킬 때 잘 학습된 모델에서 생성된 데이터로 재학습시키면 더 좋은 모델을 생성할 수 있지 않을까?==
- 예를 들어 개와 고양이를 분류하는 모델을 각각 3만장씩 넣어서 모델을 만들고, VAE를 통해 개와 고양이를 학습한 모델에서 개와 고양이 중간 데이터를 넣으면 과연 이전 분류 모델이 개인지 고양이인지 판별할 수 있을까?
- 어디까지 개이고, 어디까지 고양이라고 분류할까?

베이즈 이론
우리가 틀렸을 가능성을 생각해야 한다.
사전 확률에 확신하면 새로운 정보가 기존에 가진 지식(사전 확률)을 바꿀 수 없다.