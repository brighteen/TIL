정보이론에서 어떤 확률 함수가 균등분포를 따를 때(모든 사건의 확률이 동일하다면) 엔트로피(정보량의 기댓치)가 최대가 된다.
3만 나오는 편향된 주사위의 엔트로피는 0

크로스 엔트로피
실제로는 p(실제 확률)를 따르지만 그 p를 모르니까 q를 따른다고 해볼게.

KL-divergence
크로스 엔트로피(내가 제대로 못 짠 평균 코드 길이) - 엔트로피(이론적으로 최대한 잘짠 평균 코드 길이) -> 양수가 나올 것.
p와 q의 차이
두 확률분포의 차이를 계산하는 데에 사용하는 함수
의사결정트리에서 정보이득(information gain)와 같은 건가?

mutual information
독립적이지 않은 정보 -> 상호간에 정보
두 정보가 독립이면 I(X; Y) = 0
I(X; Y) = H(X) + H(Y) - H(X,Y) (당연히 두 정보가 독립이면 H(X,Y) = H(X) + H(Y))
상호 정보량은 두 확률 변수 X,Y가 독립에 가까운지 여부를 평가하는 값 -> 상호의존성을 측정하는 measure

통계적 독립
두 확률분포의 결합확률 분포가 주변확률분포의 곱으로 나타낼 수 있을 때 두 확률분포는 통계적 독립이다.